{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd7scc-8QJvE"
   },
   "source": [
    "## CS441: Applied ML - HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QagOldZDQJvG"
   },
   "source": [
    "## Parts 1-2: MNIST\n",
    "\n",
    "Include all the code for generating MNIST results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 13891,
     "status": "ok",
     "timestamp": 1757986400899,
     "user": {
      "displayName": "Derek Hoiem",
      "userId": "04246498393884024615"
     },
     "user_tz": 300
    },
    "id": "vO_W4UH7NNBo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 14:43:58.446969: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-14 14:43:58.459007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-14 14:43:58.992466: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-14 14:44:02.605191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-14 14:44:02.608054: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "# initialization code\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "  '''\n",
    "  Loads, reshapes, and normalizes the data\n",
    "  '''\n",
    "  (x_train, y_train), (x_test, y_test) = mnist.load_data() # loads MNIST data\n",
    "  x_train = x_train[::-1]\n",
    "  y_train = y_train[::-1]\n",
    "  x_train = np.reshape(x_train, (len(x_train), 28*28))  # reformat to 784-d vectors\n",
    "  x_test = np.reshape(x_test, (len(x_test), 28*28))\n",
    "  maxval = x_train.max()\n",
    "  x_train = x_train/maxval  # normalize values to range from 0 to 1\n",
    "  x_test = x_test/maxval\n",
    "  return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def display_mnist(x, subplot_rows=1, subplot_cols=1):\n",
    "  '''\n",
    "  Displays one or more examples in a row or a grid\n",
    "  '''\n",
    "  if subplot_rows>1 or subplot_cols>1:\n",
    "    fig, ax = plt.subplots(subplot_rows, subplot_cols, figsize=(15,15))\n",
    "    for i in np.arange(len(x)):\n",
    "      ax[i].imshow(np.reshape(x[i], (28,28)), cmap='gray')\n",
    "      ax[i].axis('off')\n",
    "  else:\n",
    "      plt.imshow(np.reshape(x, (28,28)), cmap='gray')\n",
    "      plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-PwqmwHdhpox"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: train=60000, test =10000\n"
     ]
    }
   ],
   "source": [
    "# example of using MNIST load and display  functions\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "# display_mnist(x_train[:10],1,10)\n",
    "print('Total size: train={}, test ={}'.format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynniUqsGRhxv"
   },
   "source": [
    "1. Retrieval, Clustering, and NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLgMfk9GLalv"
   },
   "source": [
    "**Retrieval**: Implement the function get_nearest using Euclidean (L2) distance. Check that get_nearest(x_test[0], x_train) returns i=6156. Report the index of the closest example in x_train to x_test[1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XAXN7xFwl3al"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6156\n",
      "31117\n"
     ]
    }
   ],
   "source": [
    "# Retrieval\n",
    "\n",
    "def get_nearest(X_query, X):\n",
    "  ''' Return the index of the sample in X that is closest to X_query according\n",
    "      to L2 distance '''\n",
    "  # TO DO\n",
    "  distances = np.linalg.norm(X - X_query, axis=1)\n",
    "  closest = np.argmin(distances)\n",
    "  return closest\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "j = get_nearest(x_test[0], x_train)\n",
    "print(j)\n",
    "j = get_nearest(x_test[1], x_train)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYIo5TkbLjSp"
   },
   "source": [
    "**K-means**: Using your get_nearest function, write a function kmeans that iteratively assigns each data point to the nearest cluster center.  Apply it to only the first 1000 examples, x_train[:1000]. Try this with K = 10 and K = 30, and display the cluster centers after each iteration. Include the displays from after the 1st and 10th iteration for K=30 in your report. See the note in the assignment if your cluster centers do not seem to be changing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CqJcR391H62S"
   },
   "outputs": [],
   "source": [
    "# K-means\n",
    "\n",
    "def kmeans(X, K, niter=10):\n",
    "  '''\n",
    "  Starting with the first K samples in X as cluster centers, iteratively assign each\n",
    "  point to the nearest cluster and compute the mean of each cluster.\n",
    "  Input: X[i] is the ith sample, K is the number of clusters, niter is the number of iterations\n",
    "  Output: K cluster centers\n",
    "  '''\n",
    "  # TO DO -- implement kmeans and add code to display cluster centers at each iteration\n",
    "\n",
    "  cs = X[:K].copy()\n",
    "  for i in range(niter):\n",
    "    labels = np.array([get_nearest(x, cs) for x in X])\n",
    "    cs = np.array([X[labels == i].mean(axis=0) for i in range(K)])\n",
    "\n",
    "    if i == 1 or i == niter-1:\n",
    "      display_mnist(cs, 1, K)\n",
    "\n",
    "  return cs\n",
    "\n",
    "\n",
    "# K=30\n",
    "# centers = kmeans(x_train[:1000], K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv9gv8aJL8k6"
   },
   "source": [
    "**1-NN**: Now, use your get_nearest function to perform 1-nearest neighbor. For each test sample, find the index of the closest sample in the training data to predict its label. To check your method, calculate the error for the first 100 test samples using only the first 1,000 training samples; the error should be 19%. Report the percent error for the first 100 test samples using the first 10,000 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vR86oOAYR9tY"
   },
   "outputs": [],
   "source": [
    "# 1-NN\n",
    "\n",
    "# TO DO\n",
    "def one_nn(x, X):\n",
    "    return get_nearest(x, X)\n",
    "\n",
    "\n",
    "def calc_error(X_train, Y_train, X_test, Y_test):\n",
    "    correct = 0\n",
    "    false = 0\n",
    "    for i, x in enumerate(X_test):\n",
    "        y = Y_train[one_nn(x, X_train)]\n",
    "        y_true = Y_test[i]\n",
    "\n",
    "        if y == y_true:\n",
    "            correct += 1\n",
    "        else:\n",
    "            false += 1\n",
    "\n",
    "    return (false/correct)*100\n",
    "\n",
    "# check = (calc_error(x_train[:1000], y_train[:1000], x_test, y_test))\n",
    "\n",
    "# if abs(check - 19) > 1:\n",
    "#     print(f\"Check value got {check}% instead of 19%, possible error\")\n",
    "\n",
    "# work = calc_error(x_train[:10000], y_train[:10000], x_test[:100], y_test[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check value: 19.402985074626866%\n",
      "work value: 7.526881720430108%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Check value: {check}%\")\n",
    "print(f\"work value: {work}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAOJiY7_Uvkr"
   },
   "source": [
    "2. Make it fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yn6HRS_ZUC9s"
   },
   "outputs": [],
   "source": [
    "# install libraries you need for part 2\n",
    "# !apt install libomp-dev\n",
    "# !pip install faiss-cpu\n",
    "import faiss\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc46GxJmMWbN"
   },
   "source": [
    "**Retrieval**: Exact search can be performed using the code below.\n",
    "```\n",
    "index = faiss.IndexFlatL2(X.shape[1])  # set for exact search\n",
    "index.add(x_train) # add the data\n",
    "dist, idx = index.search(x_test[:2],1) # returns index and sq err for each sample\n",
    "```\n",
    "Check that idx matches your retrieved indices from Part 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nTNoXRtDU-hW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.0398464]\n",
      " [20.79831  ]] [[ 6156]\n",
      " [31117]]\n"
     ]
    }
   ],
   "source": [
    "# retrieval\n",
    "\n",
    "# TO DO (check that you're using FAISS correctly)\n",
    "index = faiss.IndexFlatL2(x_train.shape[1])\n",
    "index.add(x_train)\n",
    "dist, idx = index.search(x_test[:2], 1)\n",
    "\n",
    "print(dist, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JO8HK1o-MmDC"
   },
   "source": [
    "**K-means**: Complete kmeans_fast using FAISS for the retrieval instead of your get_nearest function. In each iteration, create a new index, add the cluster centers, and find the nearest center to all samples.  Once it's working, disable any print or display functions inside the kmeans loop.\n",
    "\n",
    "Record the root mean squared error (RMSE) at the start of each iteration, and plot the RMSE for each iteration for K=10, K=30, and K=100 when clustering the full training set with 20 iterations. The RMSE for each data point is based on its distance from its assigned cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6OP5UnkvPs65"
   },
   "outputs": [],
   "source": [
    "# K-means\n",
    "\n",
    "def get_nearest_faiss(x_query, X):\n",
    "  index = faiss.IndexFlatL2(X.shape[1])\n",
    "  index.add(X)\n",
    "  dist, idx = index.search(np.array([x_query]), 1)\n",
    "  return dist, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kmeans_fast(X, K, niter=10):\n",
    "  '''\n",
    "  Starting with the first K samples in X as cluster centers, iteratively assign each\n",
    "  point to the nearest cluster using faiss and compute the mean of each cluster.\n",
    "  Input: X[i] is the ith sample, K is the number of clusters, niter is the number of iterations\n",
    "  Output: K cluster centers\n",
    "  '''\n",
    "\n",
    "  # TO DO (you can base this on part 1, but use FAISS for search)\n",
    "  # if you include display code, you need to re-organize the plotting code below\n",
    "\n",
    "  cs = X[:K].copy()\n",
    "  for _ in range(niter):\n",
    "    tups = np.array([get_nearest_faiss(x, cs) for x in X])\n",
    "    errs, labels = np.array([(e[0][0][0], e[1][0][0]) for e in tups]).T   \n",
    "    cs = np.array([X[labels == i].mean(axis=0) for i in range(K)])\n",
    "\n",
    "  return cs, errs\n",
    "\n",
    "K=10\n",
    "centers, rmse = kmeans_fast(x_train, K, niter=20)\n",
    "plt.plot(np.arange(len(rmse)), rmse, label='K=10')\n",
    "\n",
    "K=30\n",
    "centers, rmse = kmeans_fast(x_train, K, niter=20)\n",
    "plt.plot(np.arange(len(rmse)), rmse, label='K=30')\n",
    "\n",
    "K=100\n",
    "centers, rmse = kmeans_fast(x_train, K, niter=20)\n",
    "plt.plot(np.arange(len(rmse)), rmse, label='K=100')\n",
    "plt.legend(), plt.ylabel('RMSE'), plt.xlabel('# iterations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20GcELO6M2jS"
   },
   "source": [
    "**1-NN**: Use FAISS to evaluate 1-NN on the full training and test sets.  Try this with both the exact search and LSH approximate search.  The only difference is how you set up the index.  \n",
    "\n",
    "For LSH, use:\n",
    "```\n",
    "dim = X.shape[1]\n",
    "index = faiss.IndexLSH(dim, dim)\n",
    "```\n",
    "\n",
    "Evaluate 1-NN using each search method, while varying the number of training samples: s in [100,1000, 10000, 60000].  In each case, use x_train[:s] as the train set.  Plot number of samples vs. percent error on a semilogx plot for both exact and LSH (on the same plot).  Also, record timings using `time.time()` and plot samples vs. time on a semilogx plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8gny8K050x6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-NN\n",
    "\n",
    "def one_nn_exact(x, X):\n",
    "    return get_nearest_faiss(x, X)\n",
    "\n",
    "def one_nn_lsh(x, X):\n",
    "    dim = X.shape[1]\n",
    "    index = faiss.IndexLSH(dim, dim)\n",
    "    index.add(X)\n",
    "    dist, idx = index.search(np.array([x]), 1)\n",
    "    return dist, idx\n",
    "\n",
    "nsample = [100, 1000, 10000, 60000]\n",
    "acc_exact = []\n",
    "acc_lsh = []\n",
    "timing_exact = 0\n",
    "timing_lsh = 0\n",
    "\n",
    "for s in nsample:\n",
    "    ae = 0\n",
    "    al = 0\n",
    "\n",
    "    dim = x_train[:s].shape[1]\n",
    "\n",
    "    index_lsh = faiss.IndexLSH(dim, dim)\n",
    "    index_lsh.add(x_train[:s])\n",
    "\n",
    "    index_exact = faiss.IndexFlatL2(dim)\n",
    "    index.add(x_train[:s])\n",
    "\n",
    "    for i, tv in enumerate(x_test):\n",
    "       exact_d, exact_l = index_exact.search(np.array([tv]), 1)\n",
    "       lsh_d, lsh_l = index_lsh.search(np.array([tv]), 1)\n",
    "\n",
    "       act = y_test[i]\n",
    "\n",
    "       if y_test[exact_l.flatten()[0]] != act:\n",
    "           al += 1\n",
    "       if y_test[lsh_l.flatten()[0]] != act:\n",
    "           ae += 1\n",
    "    print(f\"{s} done\\n\")\n",
    "    acc_exact.append(ae)\n",
    "    acc_lsh.append(al)\n",
    "\n",
    "print(acc_lsh, acc_exact)\n",
    "\n",
    "\n",
    "# TO DO\n",
    "\n",
    "# plt.semilogx(nsample, (1-acc_exact)*100, label='exact')\n",
    "# plt.semilogx(nsample, (1-acc_lsh)*100, label='lsh')\n",
    "# plt.legend(), plt.ylabel('% error'), plt.xlabel('# training samples')\n",
    "# plt.show()\n",
    "\n",
    "# plt.semilogx(nsample, timing_exact, label='exact')\n",
    "# plt.semilogx(nsample, timing_lsh, label='lsh')\n",
    "# plt.legend(), plt.ylabel('time'), plt.xlabel('# training samples')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t14GcS2vNQlV"
   },
   "source": [
    "In your report, indicate which label is most often confused with ‘3’ when using the full training set and exact search for 1-NN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEa4r-nShBLQ"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import sklearn\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F49MYTryhPJB"
   },
   "source": [
    "## Part 3: Temperature Regression\n",
    "\n",
    "Include all your code used for part 3 in this section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PG1tNzr1tSO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import drive\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# load data (modify to match your data directory or comment)\n",
    "def load_temp_data():\n",
    "  drive.mount('/content/drive')\n",
    "  datadir = \"/content/drive/My Drive/CS441/hw1/\"\n",
    "  T = np.load(datadir + 'temperature_data.npz')\n",
    "  xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day = \\\n",
    "  T['x_train'], T['y_train'], T['x_val'], T['y_val'], T['x_test'], T['y_test'], T['dates_train'], T['dates_val'], T['dates_test'], T['feature_to_city'], T['feature_to_day']\n",
    "  return (xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day)\n",
    "\n",
    "# plot one data point for listed cities and target date\n",
    "def plot_temps(x, y, cities, feature_to_city, feature_to_day, target_date):\n",
    "  nc = len(cities)\n",
    "  ndays = 5\n",
    "  xplot = np.array([-5,-4,-3,-2,-1])\n",
    "  yplot = np.zeros((nc,ndays))\n",
    "  for f in np.arange(len(x)):\n",
    "    for c in np.arange(nc):\n",
    "      if cities[c]==feature_to_city[f]:\n",
    "        yplot[feature_to_day[f]+ndays,c] = x[f]\n",
    "  plt.plot(xplot,yplot)\n",
    "  plt.legend(cities)\n",
    "  plt.plot(0, y, 'b*', markersize=10)\n",
    "  plt.title('Predict Temp for Cleveland on ' + target_date)\n",
    "  plt.xlabel('Day')\n",
    "  plt.ylabel('Avg Temp (C)')\n",
    "  plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ6shFfVHlpZ"
   },
   "outputs": [],
   "source": [
    "# load data (use xt and yt so that we aren't replacing the MNIST variables)\n",
    "(xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day) = load_temp_data()\n",
    "''' Data format:\n",
    "      x_train, y_train: features and target value for each training sample (used to fit model)\n",
    "      x_val, y_val: features and target value for each validation sample (used to select hyperparameters, such as regularization and K)\n",
    "      x_test, y_test: features and target value for each test sample (used to evaluate final performance)\n",
    "      dates_xxx: date of the target value for the corresponding sample\n",
    "      feature_to_city: maps from a feature index to the city\n",
    "      feature_to_day: maps from a feature index to a day relative to the target value, e.g. -2 means two days before\n",
    "      Note: 361 is the feature index for the temperature of Cleveland on the previous day\n",
    "'''\n",
    "f = 361\n",
    "print('Feature {}: city = {}, day= {}'.format(f,feature_to_city[f], feature_to_day[f]))\n",
    "baseline_rmse = np.sqrt(np.mean((yt_val[1:]-yt_val[:-1])**2)) # root mean squared error example\n",
    "print('Baseline - prediction using previous day: RMSE={}'.format(baseline_rmse))\n",
    "\n",
    "# plot first two x/y for val\n",
    "plot_temps(xt_val[0], yt_val[0], ['Cleveland', 'New York', 'Chicago', 'Denver', 'St. Louis'], feature_to_city, feature_to_day, dates_val[0])\n",
    "plot_temps(xt_val[1], yt_val[1], ['Cleveland', 'New York', 'Chicago', 'Denver', 'St. Louis'], feature_to_city, feature_to_day, dates_val[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27hxeSZdNVH0"
   },
   "source": [
    "**KNN Regression**: Perform 5-NN regression, reporting RMSE for two variants:\n",
    "\n",
    "\n",
    "1.   Original features\n",
    "2.   Normalize the features by subtracting the previous day’s Cleveland temperature. I.e., if previous day’s Cleveland temperature is `c`, features are `X`, and value to predict is `y`, then predict `y_query-c = NN(X_query, X-c, y-c)`\n",
    "\n",
    "For these experiments, train on (xt_train, yt_train) and test on (xt_test, yt_test). To validate your method, if you set `K=3`, you should get an RMSE of `3.314` for the original features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERcv0wIcQnWB"
   },
   "outputs": [],
   "source": [
    "# K-NN Regression\n",
    "\n",
    "def regress_KNN(X_trn, y_trn, X_tst, K=1):\n",
    "  '''\n",
    "  Predict the target value for each data point in X_tst using a\n",
    "  K-nearest neighbor regressor based on (X_trn, y_trn), with L2 distance.\n",
    "  Input: X_trn[i] is the ith training data. y_trn[i] is the ith training label. K is the number of closest neighbors to use.\n",
    "  Output: return y_pred, where y_pred[i] is the predicted ith test value\n",
    "  '''\n",
    "  # TO DO\n",
    "\n",
    "def normalize_features(x, y, fnum):\n",
    "  ''' Normalize the features in x and y.\n",
    "      For each data sample i:\n",
    "        x2[i] = x[i]-x[i,fnum]\n",
    "        y2[i] = y[i]-x[i,fnum]\n",
    "  '''\n",
    "  # TO DO\n",
    "\n",
    "\n",
    "\n",
    "# KNN with original features\n",
    "\n",
    "# TO DO\n",
    "\n",
    "# KNN with normalized features\n",
    "fnum = 361 # previous day temp in Cleveland\n",
    "\n",
    "# TO DO\n",
    "\n",
    "# KNN with normalized features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3X3j_efPhh6e"
   },
   "source": [
    "## Part 5: Stretch Goals\n",
    "Include all your code used for part 5 in this section. You can copy-paste code from parts 1-3 if it is re-usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxIi_KTYOTkf"
   },
   "source": [
    "Compare K-NN on the MNIST classification for N=1, 3, 5, 11, 25. For these tests, use x_train[:50000] as a training set and x_train[50000:] as a validation set.  Report error on the validation set for all parameters. Then performance on the test set for the best parameter using the full training set.\n",
    "\n",
    "When K is greater than 1, return the most common label of the nearest samples.  *If there \tis a tie, return the most common label with the closest sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyZ9L4zXd0iO"
   },
   "outputs": [],
   "source": [
    "# Stretch: KNN classification (Select K)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1nK3-QhOnrp"
   },
   "source": [
    "Compare K-NN on the temperature regression dataset for N=1, 3, 5, 11, 25 using both feature types. Report all results on the validation set, and then run the single best setting on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wD6B7KagKlJ2"
   },
   "outputs": [],
   "source": [
    "# Stretch: KNN regression (Select K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USghVHKI3Oms"
   },
   "outputs": [],
   "source": [
    "# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n",
    "# For use in Colab.  For local, just use jupyter nbconvert directly\n",
    "\n",
    "import os\n",
    "# @title Convert Notebook to PDF. Save Notebook to given directory\n",
    "NOTEBOOKS_DIR = \"/content/drive/MyDrive/CS441/hw1\" # @param {type:\"string\"}\n",
    "NOTEBOOK_NAME = \"CS441_HW1_Solution.ipynb\" # @param {type:\"string\"}\n",
    "#------------------------------------------------------------------------------#\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\", force_remount=True)\n",
    "NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n",
    "assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n",
    "!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n",
    "!apt install pandoc > /dev/null 2>&1\n",
    "!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n",
    "NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n",
    "assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n",
    "print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1EyDprvfSjGmR5oM4k0IFTPYLEFzQIpAz",
     "timestamp": 1673299853915
    },
    {
     "file_id": "1-roWT29Q7bvNPuwnejXmlMBhymBQnKfm",
     "timestamp": 1673043755861
    }
   ]
  },
  "kernelspec": {
   "display_name": "appliedml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
