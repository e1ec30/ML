{"cells":[{"cell_type":"markdown","metadata":{"id":"Gd7scc-8QJvE"},"source":["## CS441: Applied ML - HW 2"]},{"cell_type":"markdown","metadata":{"id":"QagOldZDQJvG"},"source":["## Parts 1-2: MNIST\n","\n","Include all the code for generating MNIST results below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vO_W4UH7NNBo"},"outputs":[],"source":["# initialization code\n","import numpy as np\n","from keras.datasets import mnist\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","from scipy import stats\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","def load_mnist():\n","  '''\n","  Loads, reshapes, and normalizes the data\n","  '''\n","  (x_train, y_train), (x_test, y_test) = mnist.load_data() # loads MNIST data\n","  x_train = x_train[::-1]\n","  y_train = y_train[::-1]\n","  x_train = np.reshape(x_train, (len(x_train), 28*28))  # reformat to 784-d vectors\n","  x_test = np.reshape(x_test, (len(x_test), 28*28))\n","  maxval = x_train.max()\n","  x_train = x_train/maxval  # normalize values to range from 0 to 1\n","  x_test = x_test/maxval\n","  return (x_train, y_train), (x_test, y_test)\n","\n","def display_mnist(x, subplot_rows=1, subplot_cols=1):\n","  '''\n","  Displays one or more examples in a row or a grid\n","  '''\n","  if subplot_rows>1 or subplot_cols>1:\n","    fig, ax = plt.subplots(subplot_rows, subplot_cols, figsize=(15,15))\n","    for i in np.arange(len(x)):\n","      ax[i].imshow(np.reshape(x[i], (28,28)), cmap='gray')\n","      ax[i].axis('off')\n","  else:\n","      plt.imshow(np.reshape(x, (28,28)), cmap='gray')\n","      plt.axis('off')\n","  plt.show()"]},{"cell_type":"markdown","source":["### Part 1: PCA and Data Compression\n","\n","Compute the principal components using sklearn.decomposition.PCA over the full training set.  "],"metadata":{"id":"vCQCNk2vfG-D"}},{"cell_type":"markdown","source":["a. Display the first 10 principal components using the same\n","tool that is used to display centroids (`display_mnist`)."],"metadata":{"id":"6xxQobwiQG5_"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","(x_train, y_train), (x_test, y_test) = load_mnist()\n","\n","# Compute the first 10 principal components using x_train\n","# Include random_state=0 as an argument when initializing PCA\n","\n","# TO DO\n","\n","# Display First 10 Components\n","\n"],"metadata":{"id":"mev8hWiie5qz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["b. Scatterplot the first two dimensions of PCA-transformed x_train[:500]. Show a different color for each digit label."],"metadata":{"id":"1JVqYwaQQNf1"}},{"cell_type":"code","source":["# Scatter plot of first two PCA dimensions\n","import seaborn as sns\n","\n","# use pca.transform\n","# TO DO\n","ind = np.arange(500)\n","sns.scatterplot(x=x[ind,0],y=x[ind,1], hue=y_train[ind], palette=\"colorblind\")"],"metadata":{"id":"YU8tfhSFiNPf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["c. Plot cumulative explained variance (explained_variance_ratio_) of all components."],"metadata":{"id":"zGsiEWdzQSGS"}},{"cell_type":"code","source":["# Plot cumulative explained variance ratio\n","# cumsum and pca.explained_variance_ratio_ will be useful\n","\n","# TO DO"],"metadata":{"id":"GCjjqo-HiQAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["d. Select the smallest number M of principal components that explains at least 90% of variance. Then, compress the training data and the test data using the first M principal components, and compute the total time and test error for 1-NN using the brute force Faiss method. Fit PCA on the training data only, then use the same components to transform both training and test data. Compare time and error for 1-NN using original vs compressed features."],"metadata":{"id":"OiyeWUvFQUp_"}},{"cell_type":"code","source":["# Select number of dimensions that explains 90% of variance, according to your plot above\n","!apt install libomp-dev > /dev/null 2>&1\n","!pip install faiss-cpu > /dev/null 2>&1\n","import faiss\n","import time\n","\n","# Get time and error when using original features with brute force 1-NN\n","# TO DO\n","\n","# Get time and error when using compressed features with brute force 1-NN\n","# TO DO\n","\n","\n"],"metadata":{"id":"tsuSefVkiSPx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Part 2: MNIST Classification with Linear Models\n","\n","We revisit classification using linear logistic regression (LLR) and SVM. For logistic regression, use `sklearn.linear_model.LogisticRegression` with default parameters (unless stated otherwise), except `max_iter=500`. For SVM, use `sklearn.svm.LinearSVC` with default parameters, except `max_iter=500`.  Do not use `svm.SVC` since that is a non-linear SVM by default.  With  `max_iter=500`, it's ok if you get a warning that it hasnâ€™t converged.  "],"metadata":{"id":"6HD8MqG0u7sY"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn import svm"],"metadata":{"id":"9K4eQYu3vlHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["a. LLR/SVM vs training size: For varying size training sets N in [100, 1000, 10000, 60000], compare and tabulate classification error for linear logistic regression and linear SVM, using x_train[:N] and x_test.    "],"metadata":{"id":"XPmtpheCIoKx"}},{"cell_type":"code","source":["# LLR\n","# TO DO\n"],"metadata":{"id":"OmDqjTgPvG-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SVM\n","# TO DO"],"metadata":{"id":"tkGeEgpfx3GK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["b. Error visualization: For each LLR and SVM, for each label (0 to 9), display the sample with the highest score for the *correct label*. E.g., out of all samples with a true label of 0, select the one that has the highest score for label 0. Display the 10 samples in a row for each method, showing the easy cases for each model.  Then, for each label, display the sample with the lowest score for the correct label. Again, display the 10 samples in a row, showing the difficult cases."],"metadata":{"id":"OTGulHAWIrsu"}},{"cell_type":"code","source":["# to get scores for logistic regression use: scores = model_lr.predict_proba(x_test)\n","# TO DO\n","\n","# to get scores for SVM use: scores = model_svm.decision_function(x_test)\n","# TO DO"],"metadata":{"id":"iRDphHMJIy6J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["c. Parameter selection: For linear SVM, use validation experiments (testing on the validation set) to estimate the best regularization strength `C`. Then evaluate a model trained with that C on the test set. For the sake of speed, in this section, use  `x_train[:1000]` as the training set for all experiments, with `max_iter=1000`.  For the validation set, use `x_train[50000:]`.\n","\n","In selection, try to get the best result on the validation set within 0.1%.  To select `C`, start with the default and increase/decrease by a factor of 2 until results either level out or start to get worse. Then, you can try half way between the most two best values. E.g. try `C` in [0.25, 0.5, 1, 2, 4].  Suppose 0.5 and 1 give the best results.  Then, try 0.75.  Once the difference between the two best results is less than 0.1%, you can stop. You can also search outside of this range if 0.25 or 4 gives the best result.  Plot the validation error for each `C` value tested on a semilogx plot, and record the best in a table.\n","\n","Note that you do not need to have a script that automatically searches over `C`.  You can try some values and manually choose the next `C` to try.  But you need to plot all the tested `C` and corresoning validation error together, so be sure to record the results."],"metadata":{"id":"1ya3HfpiIujE"}},{"cell_type":"code","source":["# Try multiple C parameters, select one that minimizes validation error\n","# Often, you need to try a few values and see those results to determine what other values to try\n","\n","# TO DO"],"metadata":{"id":"uGHnC5g6zQmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get test result for selected parameter\n","\n","# TO DO"],"metadata":{"id":"G2k-P3c7ILIy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 3: Temperature Regression\n","\n","We will investigate using linear regression for both prediction and feature selection.\n"],"metadata":{"id":"F49MYTryhPJB"}},{"cell_type":"code","source":["import numpy as np\n","from google.colab import drive\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","\n","# load data (modify to match your data directory or comment)\n","def load_temp_data():\n","  drive.mount('/content/drive')\n","  datadir = \"/content/drive/My Drive/CS441/hw1/\"\n","  T = np.load(datadir + 'temperature_data.npz')\n","  xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day = \\\n","  T['x_train'], T['y_train'], T['x_val'], T['y_val'], T['x_test'], T['y_test'], T['dates_train'], T['dates_val'], T['dates_test'], T['feature_to_city'], T['feature_to_day']\n","  return (xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day)\n","\n","# plot one data point for listed cities and target date\n","def plot_temps(x, y, cities, feature_to_city, feature_to_day, target_date):\n","  nc = len(cities)\n","  ndays = 5\n","  xplot = np.array([-5,-4,-3,-2,-1])\n","  yplot = np.zeros((nc,ndays))\n","  for f in np.arange(len(x)):\n","    for c in np.arange(nc):\n","      if cities[c]==feature_to_city[f]:\n","        yplot[feature_to_day[f]+ndays,c] = x[f]\n","  plt.plot(xplot,yplot)\n","  plt.legend(cities)\n","  plt.plot(0, y, 'b*', markersize=10)\n","  plt.title('Predict Temp for Cleveland on ' + target_date)\n","  plt.xlabel('Day')\n","  plt.ylabel('Avg Temp (C)')\n","  plt.show()\n"],"metadata":{"id":"4PG1tNzr1tSO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load data\n","(xt_train, yt_train, xt_val, yt_val, xt_test, yt_test, dates_train, dates_val, dates_test, feature_to_city, feature_to_day) = load_temp_data()\n","''' Data format:\n","      x_train, y_train: features and target value for each training sample (used to fit model)\n","      x_val, y_val: features and target value for each validation sample (used to select hyperparameters, such as regularization and K)\n","      x_test, y_test: features and target value for each test sample (used to evaluate final performance)\n","      dates_xxx: date of the target value for the corresponding sample\n","      feature_to_city: maps from a feature number to the city\n","      feature_to_day: maps from a feature number to a day relative to the target value, e.g. -2 means two days before\n","      Note: 361 is the temperature of Cleveland on the previous day\n","'''\n","f = 361\n","print('Feature {}: city = {}, day= {}'.format(f,feature_to_city[f], feature_to_day[f]))\n","baseline_rmse = np.sqrt(np.mean((yt_val[1:]-yt_val[:-1])**2)) # root mean squared error\n","print('Baseline - prediction using previous day: RMSE={}'.format(baseline_rmse))\n","\n","# plot first two x/y for val\n","plot_temps(xt_val[0], yt_val[0], ['Cleveland', 'New York', 'Chicago', 'Denver', 'St. Louis'], feature_to_city, feature_to_day, dates_val[0])\n","plot_temps(xt_val[1], yt_val[1], ['Cleveland', 'New York', 'Chicago', 'Denver', 'St. Louis'], feature_to_city, feature_to_day, dates_val[1])\n","\n"],"metadata":{"id":"zZ6shFfVHlpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["a. Train and Test Linear Regression (LR):\n","Report RMSE results for L2-regularized linear regression (`sklearn.linear_model.Ridge`)  using default regularization parameters on the temperature regression dataset, using original and normalized features (based on feature number 361, as defined in HW1). When normalizing, remember to normalize both the training and the test sets."],"metadata":{"id":"LsZek54BhdNu"}},{"cell_type":"code","source":["def normalize_features(x, y, fnum):\n","  ''' Normalize the features in x and y.\n","      For each data sample i:\n","        x2[i] = x[i]-x[i,fnum]\n","        y2[i] = y[i]-x[i,fnum]\n","  '''\n","  x2 = x.copy()\n","  y2 = y.copy()\n","  for i in np.arange(len(x)):\n","    x2[i] = x[i] - x[i, fnum]\n","    y2[i] = y[i] - x[i,fnum]\n","  return x2, y2"],"metadata":{"id":"D4_DvqmuqM_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # linear regression (use Ridge)\n","\n","# original features\n","# TO DO\n","\n","# normalized features\n","# TO DO"],"metadata":{"id":"gTYtRTswhXz5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["b. Feature selection: Use original features for this part.  Identify the most important features by: (1) fitting a L1 Linear Regression model (`sklearn.linear_model.Lasso`) with default parameters; and (2) selecting the features that have coefficient magnitudes greater than 0.001 (`model.coef_` is the linear coefficient vector; -0.02 and 0.02 both have magnitude greater than 0.001).  For the top 10 features, report their indices and corresponding city and day. Also report the test RMSE error rates of L2 (ridge) linear regression when trained using only the top 10 selected features."],"metadata":{"id":"t9ISOklnj5po"}},{"cell_type":"code","source":["# feature analysis (select important features using Lasso)\n","# TO DO\n","\n","# predict using best features\n","# TO DO"],"metadata":{"id":"SFbDUe1bj5De"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 4: Stretch Goals\n","Include all your code used for any stretch goals in this section. Add headings where appropriate."],"metadata":{"id":"3X3j_efPhh6e"}},{"cell_type":"code","source":["# TO DO (optional)"],"metadata":{"id":"UFZfuCqJqhLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n","# install can take a minute\n","\n","import os\n","# @title Convert Notebook to PDF. Save Notebook to given directory\n","NOTEBOOKS_DIR = \"/content/drive/MyDrive/CS441/hw2\" # @param {type:\"string\"}\n","NOTEBOOK_NAME = \"CS441_HW2_Solution.ipynb\" # @param {type:\"string\"}\n","#------------------------------------------------------------------------------#\n","from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)\n","NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n","assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n","!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n","!apt install pandoc > /dev/null 2>&1\n","!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n","NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n","assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n","print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"],"metadata":{"id":"5T2IRr7fz6ta"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"16NPS0eAXdrha1CvrTipxKhNRXIQRghda","timestamp":1705680043685},{"file_id":"1EyDprvfSjGmR5oM4k0IFTPYLEFzQIpAz","timestamp":1673299853915},{"file_id":"1-roWT29Q7bvNPuwnejXmlMBhymBQnKfm","timestamp":1673043755861}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}